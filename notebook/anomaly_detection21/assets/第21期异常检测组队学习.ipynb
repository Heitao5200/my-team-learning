{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Datawhale第21期组队学习-异常检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、什么是异常检测\n",
    "\n",
    "**异常检测（Outlier Detection）**，顾名思义，是识别与正常数据不同的数据，与预期行为差异大的数据。\n",
    "\n",
    "识别如信用卡欺诈，工业生产异常，网络流里的异常（网络侵入）等问题，针对的是少数的事件。\n",
    "\n",
    "\n",
    "\n",
    "### 1.1 异常的类别\n",
    "\n",
    "**点异常**：指的是少数个体实例是异常的，大多数个体实例是正常的，例如正常人与病人的健康指标；\n",
    "\n",
    "**上下文异常**：又称上下文异常，指的是在特定情境下个体实例是异常的，在其他情境下都是正常的，例如在特定时间下的温度突然上升或下降，在特定场景中的快速信用卡交易；\n",
    "\n",
    "**群体异常**：指的是在群体集合中的个体实例出现异常的情况，而该个体实例自身可能不是异常，例如社交网络中虚假账号形成的集合作为群体异常子集，但子集中的个体节点可能与真实账号一样正常。\n",
    "\n",
    "\n",
    "\n",
    "### 1.2 异常检测任务分类\n",
    "\n",
    "**有监督**：训练集的正例和反例均有标签\n",
    "\n",
    "**无监督**：训练集无标签\n",
    "\n",
    "**半监督**：在训练集中只有单一类别（正常实例）的实例，没有异常实例参与训练\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 异常检测场景\n",
    "\n",
    "* **故障检测**\n",
    "\n",
    "* **物联网异常检测**\n",
    "\n",
    "* **欺诈检测**\n",
    "\n",
    "* **工业异常检测**\n",
    "\n",
    "* **时间序列异常检测**\n",
    "\n",
    "* **视频异常检测**\n",
    "\n",
    "* **日志异常检测**\n",
    "\n",
    "* **医疗日常检测**\n",
    "\n",
    "* **网络入侵检测**\n",
    "\n",
    "\n",
    "\n",
    "## 2、异常检测常用方法\n",
    "\n",
    "### 2.1 传统方法\n",
    "\n",
    "#### 2.1.1 基于统计学的方法\n",
    "\n",
    "统计学方法对数据的正常性做出假定。**它们假定正常的数据对象由一个统计模型产生，而不遵守该模型的数据是异常点。**统计学方法的有效性高度依赖于对给定数据所做的统计模型假定是否成立。\n",
    "\n",
    "异常检测的统计学方法的一般思想是：学习一个拟合给定数据集的生成模型，然后识别该模型低概率区域中的对象，把它们作为异常点。\n",
    "\n",
    "即利用统计学方法建立一个模型，然后考虑对象有多大可能符合该模型。\n",
    "\n",
    "假定输入数据集为$\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$，数据集中的样本服从正态分布，即$x^{(i)}\\sim N(\\mu, \\sigma^2)$，我们可以根据样本求出参数$\\mu$和$\\sigma$。\n",
    "\n",
    "$\\mu=\\frac 1m\\sum_{i=1}^m x^{(i)}$\n",
    "\n",
    "$\\sigma^2=\\frac 1m\\sum_{i=1}^m (x^{(i)}-\\mu)^2$\n",
    "\n",
    "#### 2.1.2 线性模型\n",
    "\n",
    "典型的如PCA方法，Principle Component Analysis是主成分分析，简称PCA。它的应用场景是对数据集进行降维。降维后的数据能够最大程度地保留原始数据的特征（以数据协方差为衡量标准）。\n",
    "PCA的原理是通过构造一个新的特征空间，把原数据映射到这个新的低维空间里。PCA可以提高数据的计算性能，并且缓解\"高维灾难\"。\n",
    "\n",
    "#### 2.1.3 基于相似度的方法\n",
    "\n",
    "&emsp;&emsp;这类算法适用于数据点的聚集程度高、离群点较少的情况。同时，因为相似度算法通常需要对每一个数据分别进行相应计算，所以这类算法通常计算量大，不太适用于数据量大、维度高的数据。    \n",
    "&emsp;&emsp;基于相似度的检测方法大致可以分为三类： \n",
    "\n",
    "+ 基于集群（簇）的检测，如DBSCAN等聚类算法。    \n",
    "  &emsp;&emsp;聚类算法是将数据点划分为一个个相对密集的“簇”，而那些不能被归为某个簇的点，则被视作离群点。这类算法对簇个数的选择高度敏感，数量选择不当可能造成较多正常值被划为离群点或成小簇的离群点被归为正常。因此对于每一个数据集需要设置特定的参数，才可以保证聚类的效果，在数据集之间的通用性较差。聚类的主要目的通常是为了寻找成簇的数据，而将异常值和噪声一同作为无价值的数据而忽略或丢弃，在专门的异常点检测中使用较少。    \n",
    "  &emsp;&emsp;聚类算法的优缺点：    \n",
    "  （1）能够较好发现小簇的异常；    \n",
    "  （2）通常用于簇的发现，而对异常值采取丢弃处理，对异常值的处理不够友好；    \n",
    "  （3）产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性；    \n",
    "  （4）聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。\n",
    "+ 基于距离的度量，如k近邻算法。    \n",
    "  &emsp;&emsp;k近邻算法的基本思路是对每一个点，计算其与最近k个相邻点的距离，通过距离的大小来判断它是否为离群点。在这里，离群距离大小对k的取值高度敏感。如果k太小（例如1），则少量的邻近离群点可能导致较低的离群点得分；如果k太大，则点数少于k的簇中所有的对象可能都成了离群点。为了使模型更加稳定，距离值的计算通常使用k个最近邻的平均距离。    \n",
    "  &emsp;&emsp;k近邻算法的优缺点：    \n",
    "  （1）简单；    \n",
    "  （2）基于邻近度的方法需要O(m2)时间，大数据集不适用；    \n",
    "  （3）对参数的选择敏感；   \n",
    "  （4）不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。    \n",
    "+ 基于密度的度量，如LOF（局部离群因子）算法。  \n",
    "  &emsp;&emsp;局部离群因子（LOF）算法与k近邻类似，不同的是它以相对于其邻居的局部密度偏差而不是距离来进行度量。它将相邻点之间的距离进一步转化为“邻域”，从而得到邻域中点的数量（即密度），认为密度远低于其邻居的样本为异常值。    \n",
    "  LOF（局部离群因子）算法的优缺点：    \n",
    "  （1）给出了对离群度的定量度量；    \n",
    "  （2）能够很好地处理不同密度区域的数据；   \n",
    "  （3）对参数的选择敏感。     \n",
    "\n",
    "\n",
    "\n",
    "### 2.2 集成方法\n",
    "\n",
    "集成是提高数据挖掘算法精度的常用方法。集成方法将多个算法或多个基检测器的输出结合起来。其基本思想是一些算法在某些子集上表现很好，一些算法在其他子集上表现很好，然后集成起来使得输出更加鲁棒。集成方法与基于子空间方法有着天然的相似性，子空间与不同的点集相关，而集成方法使用基检测器来探索不同维度的子集，将这些基学习器集合起来。\n",
    "\n",
    "常用的集成方法有Feature bagging，孤立森林等。\n",
    "\n",
    "**feature bagging **：\n",
    "\n",
    "与bagging法类似，只是对象是feature。\n",
    "\n",
    "\n",
    "\n",
    "**孤立森林**：\n",
    "\n",
    "孤立森林假设我们用一个随机超平面来切割数据空间，切一次可以生成两个子空间。然后我们继续用随机超平面来切割每个子空间并循环，直到每个子空间只有一个数据点为止。直观上来讲，那些具有高密度的簇需要被切很多次才会将其分离，而那些低密度的点很快就被单独分配到一个子空间了。孤立森林认为这些很快被孤立的点就是异常点。\n",
    "\n",
    "用四个样本做简单直观的理解，d是最早被孤立出来的，所以d最有可能是异常。\n",
    "\n",
    "\n",
    "\n",
    "![img](https://pic3.zhimg.com/80/v2-bb94bcf07ced88315d0a5de47677200e_720w.png)\n",
    "\n",
    "### 2.3 机器学习\n",
    "\n",
    "在有标签的情况下，可以使用树模型（gbdt,xgboost等）进行分类，缺点是异常检测场景下数据标签是不均衡的，但是利用机器学习算法的好处是可以构造不同特征。\t\n",
    "\n",
    "\n",
    "\n",
    "## 3、异常检测常用开源库\n",
    "\n",
    "**Scikit-learn：**\n",
    "\n",
    "**Scikit-learn**是一个Python语言的开源机器学习库。它具有各种分类，回归和聚类算法。也包含了一些异常检测算法，例如LOF和孤立森林。\n",
    "\n",
    "官网：https://scikit-learn.org/stable/\n",
    "\n",
    "\n",
    "\n",
    "**PyOD：**\n",
    "\n",
    "> **[Python Outlier Detection（PyOD）](https://link.zhihu.com/?target=https%3A//github.com/yzhao062/pyod)**是当下最流行的Python异常检测工具库，其主要亮点包括：\n",
    ">\n",
    "> - 包括近20种常见的异常检测算法，比如经典的LOF/LOCI/ABOD以及最新的**深度学习**如对抗生成模型（GAN）和**集成异常检测**（outlier ensemble）\n",
    "> - **支持不同版本的Python**：包括2.7和3.5+；**支持多种操作系统**：windows，macOS和Linux\n",
    "> - **简单易用且一致的API**，**只需要几行代码就可以完成异常检测**，方便评估大量算法\n",
    "> - 使用JIT和并行化（parallelization）进行优化，加速算法运行及扩展性（scalability），可以处理大量数据\n",
    ">\n",
    "> ​                                                                                       ——https://zhuanlan.zhihu.com/p/58313521\n",
    "\n",
    "\n",
    "\n",
    "## 4、练习\n",
    "\n",
    "**1、学习pyod库基本操作**\n",
    "\n",
    "（如何生成toy example，了解训练以及预测的api）\n",
    "\n",
    "参考资料：\n",
    "\n",
    "* https://zhuanlan.zhihu.com/p/58313521（pyod库作者对pyod库的介绍）\n",
    "\n",
    "* https://pyod.readthedocs.io/en/latest/ （Pyod库官网）\n",
    "\n",
    "**关于Datawhale**：\n",
    "\n",
    ">Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测——基于统计学的方法\n",
    "\n",
    "**主要内容包括：**\n",
    "\n",
    "- **高斯分布**\n",
    "- **箱线图**\n",
    "\n",
    "[TOC]\n",
    "\n",
    "\n",
    "\n",
    "## 1、概述\n",
    "\n",
    "统计学方法对数据的正常性做出假定。**它们假定正常的数据对象由一个统计模型产生，而不遵守该模型的数据是异常点。**统计学方法的有效性高度依赖于对给定数据所做的统计模型假定是否成立。\n",
    "\n",
    "异常检测的统计学方法的一般思想是：学习一个拟合给定数据集的生成模型，然后识别该模型低概率区域中的对象，把它们作为异常点。\n",
    "\n",
    "即利用统计学方法建立一个模型，然后考虑对象有多大可能符合该模型。\n",
    "\n",
    "根据如何指定和学习模型，异常检测的统计学方法可以划分为两个主要类型：参数方法和非参数方法。\n",
    "\n",
    "**参数方法**假定正常的数据对象被一个以$\\Theta$为参数的参数分布产生。该参数分布的概率密度函数$f(x,\\Theta)$给出对象$x$被该分布产生的概率。该值越小，$x$越可能是异常点。\n",
    "\n",
    "**非参数方法**并不假定先验统计模型，而是试图从输入数据确定模型。非参数方法通常假定参数的个数和性质都是灵活的，不预先确定（所以非参数方法并不是说模型是完全无参的，完全无参的情况下从数据学习模型是不可能的）。\n",
    "\n",
    "## 2、参数方法\n",
    "\n",
    "**2.1 基于正态分布的一元异常点检测**\n",
    "\n",
    "仅涉及一个属性或变量的数据称为一元数据。我们假定数据由正态分布产生，然后可以由输入数据学习正态分布的参数，并把低概率的点识别为异常点。\n",
    "\n",
    "假定输入数据集为$\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$，数据集中的样本服从正态分布，即$x^{(i)}\\sim N(\\mu, \\sigma^2)$，我们可以根据样本求出参数$\\mu$和$\\sigma$。\n",
    "\n",
    "$\\mu=\\frac 1m\\sum_{i=1}^m x^{(i)}$\n",
    "\n",
    "$\\sigma^2=\\frac 1m\\sum_{i=1}^m (x^{(i)}-\\mu)^2$\n",
    "\n",
    "求出参数之后，我们就可以根据概率密度函数计算数据点服从该分布的概率。正态分布的概率密度函数为\n",
    "\n",
    "$p(x)=\\frac 1{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "如果计算出来的概率低于阈值，就可以认为该数据点为异常点。\n",
    "\n",
    "阈值是个经验值，可以选择在验证集上使得评估指标值最大（也就是效果最好）的阈值取值作为最终阈值。\n",
    "\n",
    "例如常用的3sigma原则中，如果数据点超过范围$(\\mu-3\\sigma, \\mu+3\\sigma)$，那么这些点很有可能是异常点。\n",
    "\n",
    "这个方法还可以用于可视化。箱线图对数据分布做了一个简单的统计可视化，利用数据集的上下四分位数（Q1和Q3）、中点等形成。异常点常被定义为小于Q1－1.5IQR或大于Q3+1.5IQR的那些数据。\n",
    "\n",
    "用Python画一个简单的箱线图：\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = np.random.randn(50000) * 20 + 20\n",
    "sns.boxplot(data=data)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**2.2 多元异常点检测**\n",
    "\n",
    "涉及两个或多个属性或变量的数据称为多元数据。许多一元异常点检测方法都可以扩充，用来处理多元数据。其核心思想是把多元异常点检测任务转换成一元异常点检测问题。例如基于正态分布的一元异常点检测扩充到多元情形时，可以求出每一维度的均值和标准差。对于第$j$维：\n",
    "\n",
    "$\\mu_j=\\frac 1m\\sum_{i=1}^m x_j^{(i)}$\n",
    "\n",
    "$\\sigma_j^2=\\frac 1m\\sum_{i=1}^m (x_j^{(i)}-\\mu_j)^2$\n",
    "\n",
    "计算概率时的概率密度函数为\n",
    "\n",
    "$p(x)=\\prod_{j=1}^n p(x_j;\\mu_j,\\sigma_j^2)=\\prod_{j=1}^n\\frac 1{\\sqrt{2\\pi}\\sigma_j}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})$\n",
    "\n",
    "这是在各个维度的特征之间相互独立的情况下。如果特征之间有相关性，就要用到多元高斯分布了。\n",
    "\n",
    "\n",
    "\n",
    "**1.3 多个特征相关，且符合多元高斯分布**\n",
    "\n",
    "$\\mu=\\frac{1}{m}\\sum^m_{i=1}x^{(i)}$\n",
    "\n",
    "$\\sum=\\frac{1}{m}\\sum^m_{i=1}(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$\n",
    "\n",
    "$p(x)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\exp \\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)\\right)$\n",
    "\n",
    "\n",
    "\n",
    "**3.使用混合参数分布**\n",
    "\n",
    "在许多情况下假定数据是由正态分布产生的。当实际数据很复杂时，这种假定过于简单，可以假定数据是被混合参数分布产生的。\n",
    "\n",
    "## 3、非参数方法\n",
    "\n",
    "在异常检测的非参数方法中，“正常数据”的模型从输入数据学习，而不是假定一个先验。通常，非参数方法对数据做较少假定，因而在更多情况下都可以使用。\n",
    "\n",
    "**例子：使用直方图检测异常点。**\n",
    "\n",
    "直方图是一种频繁使用的非参数统计模型，可以用来检测异常点。该过程包括如下两步：\n",
    "\n",
    "步骤1：构造直方图。使用输入数据（训练数据）构造一个直方图。该直方图可以是一元的，或者多元的（如果输入数据是多维的）。\n",
    "\n",
    "尽管非参数方法并不假定任何先验统计模型，但是通常确实要求用户提供参数，以便由数据学习。例如，用户必须指定直方图的类型（等宽的或等深的）和其他参数（直方图中的箱数或每个箱的大小等）。与参数方法不同，这些参数并不指定数据分布的类型。\n",
    "\n",
    "步骤2：检测异常点。为了确定一个对象是否是异常点，可以对照直方图检查它。在最简单的方法中，如果该对象落入直方图的一个箱中，则该对象被看作正常的，否则被认为是异常点。\n",
    "\n",
    "对于更复杂的方法，可以使用直方图赋予每个对象一个异常点得分。例如令对象的异常点得分为该对象落入的箱的容积的倒数。\n",
    "\n",
    "使用直方图作为异常点检测的非参数模型的一个缺点是，很难选择一个合适的箱尺寸。一方面，如果箱尺寸太小，则许多正常对象都会落入空的或稀疏的箱中，因而被误识别为异常点。另一方面，如果箱尺寸太大，则异常点对象可能渗入某些频繁的箱中，因而“假扮”成正常的。\n",
    "\n",
    "## 4、HBOS\n",
    "\n",
    "HBOS全名为：Histogram-based Outlier Score。它是一种单变量方法的组合，不能对特征之间的依赖关系进行建模，但是计算速度较快，对大数据集友好。其基本假设是数据集的每个维度相互独立。然后对每个维度进行区间(bin)划分，区间的密度越高，异常评分越低。\n",
    "\n",
    "\n",
    "\n",
    "HBOS算法流程：\n",
    "\n",
    "1.为每个数据维度做出数据直方图。对分类数据统计每个值的频数并计算相对频率。对数值数据根据分布的不同采用以下两种方法：\n",
    "\n",
    "* 静态宽度直方图：标准的直方图构建方法，在值范围内使用k个等宽箱。样本落入每个桶的频率（相对数量）作为密度（箱子高度）的估计。时间复杂度：$O(n)$\n",
    "\n",
    "* 2.动态宽度直方图：首先对所有值进行排序，然后固定数量的$\\frac{N}{k}$个连续值装进一个箱里，其\t中N是总实例数，k是箱个数；直方图中的箱面积表示实例数。因为箱的宽度是由箱中第一个值和最后一个值决定的，所有箱的面积都一样，因此每一个箱的高度都是可计算的。这意味着跨度大的箱的高度低，即密度小，只有一种情况例外，超过k个数相等，此时允许在同一个箱里超过$\\frac{N}{k}$值。\n",
    "\n",
    "  时间复杂度：$O(n\\times log(n))$\n",
    "\n",
    "\n",
    "\n",
    "2.对每个维度都计算了一个独立的直方图，其中每个箱子的高度表示密度的估计。然后为了使得最大高度为1（确保了每个特征与异常值得分的权重相等），对直方图进行归一化处理。最后，每一个实例的HBOS值由以下公式计算：\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "H B O S(p)=\\sum_{i=0}^{d} \\log \\left(\\frac{1}{\\text {hist}_{i}(p)}\\right)\n",
    "$$\n",
    "\n",
    "**推导过程**：\n",
    "\n",
    "假设样本*p*第 *i* 个特征的概率密度为$p_i(p)$ ，则*p*的概率密度可以计算为：\n",
    "$$\n",
    "P(p)=P_{1}(p) P_{2}(p) \\cdots P_{d}(p)\n",
    "$$\n",
    "两边取对数：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log (P(p)) &=\\log \\left(P_{1}(p) P_{2}(p) \\cdots P_{d}(p)\\right) =\\sum_{i=1}^{d} \\log \\left(P_{i}(p)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "概率密度越大，异常评分越小，为了方便评分，两边乘以“-1”：\n",
    "$$\n",
    "-\\log (P(p))=-1 \\sum_{i=1}^{d} \\log \\left(P_{t}(p)\\right)=\\sum_{i=1}^{d} \\frac{1}{\\log \\left(P_{i}(p)\\right)}\n",
    "$$\n",
    "最后可得：\n",
    "$$\n",
    "H B O S(p)=-\\log (P(p))=\\sum_{i=1}^{d} \\frac{1}{\\log \\left(P_{i}(p)\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 5、总结\n",
    "\n",
    "1.异常检测的统计学方法由数据学习模型，以区别正常的数据对象和异常点。使用统计学方法的一个优点是，异常检测可以是统计上无可非议的。当然，仅当对数据所做的统计假定满足实际约束时才为真。\n",
    "\n",
    "2.HBOS在全局异常检测问题上表现良好，但不能检测局部异常值。但是HBOS比标准算法快得多，尤其是在大数据集上。\n",
    "\n",
    "\n",
    "\n",
    "## 6、练习\n",
    "\n",
    "**1.使用PyOD库生成toy example并调用HBOS**\n",
    "\n",
    "\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "[1]Goldstein, M. and Dengel,  A., 2012. Histogram-based outlier score (hbos):A fast unsupervised anomaly detection algorithm . In*KI-2012: Poster and Demo Track*, pp.59-63.\n",
    "\n",
    "[2]http://speech.ee.ntu.edu.tw/~tlkagk/courses.html\n",
    "\n",
    "[3]http://cs229.stanford.edu/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**关于Datawhale**：\n",
    "\n",
    ">Datawhale是一个专注于数据科学与AI领域的开源组织，汇集了众多领域院校和知名企业的优秀学习者，聚合了一群有开源精神和探索精神的团队成员。Datawhale以“for the learner，和学习者一起成长”为愿景，鼓励真实地展现自我、开放包容、互信互助、敢于试错和勇于担当。同时Datawhale 用开源的理念去探索开源内容、开源学习和开源方案，赋能人才培养，助力人才成长，建立起人与人，人与知识，人与企业和人与未来的联结。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
