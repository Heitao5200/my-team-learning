{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task09 图神经网络的表示能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 知识回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNN通用框架包括邻域信息转换、邻域信息集合、跨层连接、输入图和特征的增广、学习目标\n",
    "- GNN训练步骤：拆分数据集、预测头、损失函数、模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 GNN的表示能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 GNN基础概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 图神经网络核心思想：节点使用神经网络聚合来自其本地网络邻域的信息生成节点嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单个GNN层：\n",
    "    - 信息计算：$m_u^{(l)} = \\text{MSG}^{(l)} (h_u^{(l - 1)})$\n",
    "    - 信息聚合：$h_v^{(l)} = \\text{AGG}^{(l)} (\\{ m_u^{(l)}, u \\in N(v) \\})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNN模型：\n",
    "    - GCN：均值池化 + 线性 + ReLU非线性\n",
    "    - GraphSAGE：MLP + 最大池化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 解决GNN无法区分计算图根节点的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNN的表达能力：等价于区分计算图根节点Embedding的能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 思路讨论：\n",
    "    - 一般来说，不同的局部邻域定义不同的计算图，计算图与每个节点周围的根节点子树结构相同。\n",
    "    - 实现思路：使用单射函数，将不同的根子树映射到不同的节点嵌入中，先获得树的单个级别的结构，然后利用递归算法得到树的整个结构\n",
    "    - 如果GNN聚合的每一步都可以完全保留相邻信息，则生成的节点嵌入可以区分不同的有根子树\n",
    "    - 理想GNN：不同的计算图根节点输出不同的Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 设计最具表现力的GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设计前提：\n",
    "    - GNN的表达能力可以通过使用邻域聚合函数来表征\n",
    "    - 更具表现力的聚合函数会使得GNN更具表现力\n",
    "    - 单射聚合函数可以得到最具表现力的GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 讨论：\n",
    "    - 邻域聚合可以抽象为多重集合（具有重复元素的集和）上的函数\n",
    "    - GCN的聚合函数（平均池化层）无法区分颜色比例相同的不同超集\n",
    "    - GraphSAGE的聚合函数无法区分不同的超集与同一集合下的不同颜色\n",
    "    - GCN和GraphSAGE并不是最强大的GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设计最具表现力的GNN\n",
    "    - 通过在多重集合上设计单射邻域聚合函数来实现\n",
    "    - 任何单射多重集合函数都能表示为$\\displaystyle \\Phi \\left( \\sum_{x \\in S} f(x) \\right )$\n",
    "    - 万能近似定理：在具有一个隐藏层的MLP模型中，当隐藏层的维度足够大，且使用非线性函数可以将任何连续函数近似到任意精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 图同构网络（Graph Isomorphism Network，GIN）：\n",
    "$$\n",
    "\\text{MLP}_{\\Phi} \\left ( \\sum_{x \\in S} \\text{MLP}_f (x) \\right )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GIN算法步骤：\n",
    "    1. 初始化每个节点的颜色$c^{(0)}(v)$\n",
    "    2. 使用如下公式，对节点的颜色进行迭代更新\n",
    "    $$\n",
    "    c^{(k+1)}(v) = \\text{HASH} \\left( c^{(k)}(v), \\{c^{(k)}(u)\\}_{u \\in N(v)} \\right)\n",
    "    $$\n",
    "    3. 迭代$K$步后，得到稳定的节点颜色$c^{(K)}(v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GIN模型：\n",
    "$$\n",
    "\\text{MLP}_{\\Phi} \\left( (1 + \\epsilon) \\cdot \\text{MLP}_f (c^{(k)}(v)) + \\sum_{u \\in N(v)} \\text{MLP}_f (c^{(k)}(u)) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果输入特征$c^{(0)}(v)$表示为独热向量，那么直接求和的函数就是单射，GIN卷积层可以表示为：\n",
    "$$\n",
    "\\text{GINConv}\\left( c^{(k)}, \\{ c^{(k)}(u)\\}_{u \\in N(v)} \\right) = \\text{MLP}_{\\Phi} \\left( (1 + \\epsilon) \\cdot c^{(k)}(v) + \\sum_{u \\in N(v)} c^{(k)}(u) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GIN可以理解为WL graph Kernel的可微神经版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GIN相对于WL图内核的优点：\n",
    "    - 节点嵌入是低维的；因此，它们可以捕获不同节点的细粒度相似性\n",
    "    - 可以根据下游任务学习优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 问题排查"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 通用解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据预处理：特征标准化处理\n",
    "- 优化器：使用ADAM优化器\n",
    "- 激活函数：ReLU通常效果很良好，可使用LeakyReLU、PReLU等其他激活函数，输出层没有激活函数，每一层包含偏置项\n",
    "- 嵌入维度：通常选择32、64和128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 调试深度网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Debug问题：损失/准确值在训练时未收敛\n",
    "    - 检查pipeline\n",
    "    - 调整学习率等超参数\n",
    "    - 注意权重参数初始化\n",
    "    - 仔细观察损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型开发：\n",
    "    - 在训练集上存在过拟合情况\n",
    "    - 检查loss曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 本章总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本次任务，主要介绍了图神经网络的表示能力，包括："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GNN表示能力：GNN的表达能力等价于区分计算图根节点Embedding的能力\n",
    "- 图同构网络（GIN）：GIN可以理解为WL graph Kernel的可微神经版本\n",
    "- 问题排查：数据预处理、优化器、激活函数、调整嵌入维度、Debug深度网络的方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
