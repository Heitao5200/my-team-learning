{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task05 PyTorch进阶训练技巧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以函数方式定义：通过输出值和目标值进行计算，返回损失值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以类方式定义：通过继承`nn.Module`，将其当做神经网络的一层来看待"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以DiceLoss损失函数为例，定义如下：\n",
    "$$\n",
    "DSC = \\frac{2|X∩Y|}{|X|+|Y|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss,self).__init__()\n",
    "        \n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()                   \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 动态调整学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scheduler：学习率衰减策略，解决学习率选择的问题，用于提高精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorch Scheduler策略：\n",
    "    - [lr_scheduler.LambdaLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR)\n",
    "    - [lr_scheduler.MultiplicativeLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeLR.html#torch.optim.lr_scheduler.MultiplicativeLR)\n",
    "    - [lr_scheduler.StepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR)\n",
    "    - [lr_scheduler.MultiStepLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiStepLR.html#torch.optim.lr_scheduler.MultiStepLR)\n",
    "    - [lr_scheduler.ExponentialLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR.html#torch.optim.lr_scheduler.ExponentialLR)\n",
    "    - [lr_scheduler.CosineAnnealingLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n",
    "    - [lr_scheduler.ReduceLROnPlateau](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau)\n",
    "    - [lr_scheduler.CyclicLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CyclicLR.html#torch.optim.lr_scheduler.CyclicLR)\n",
    "    - [lr_scheduler.OneCycleLR](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.OneCycleLR.html#torch.optim.lr_scheduler.OneCycleLR)\n",
    "    - [lr_scheduler.CosineAnnealingWarmRestarts](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用说明：需要将`scheduler.step()`放在`optimizer.step()`后面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自定义Scheduler：通过自定义函数对学习率进行修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概念：找到一个同类已训练好的模型，调整模型参数，使用数据进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型微调的流程\n",
    "  1. 在源数据集上预训练一个神经网络模型，即源模型\n",
    "  2. 创建一个新的神经网络模型，即目标模型，该模型复制了源模型上除输出层外的所有模型设计和参数\n",
    "  3. 给目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化改成的模型参数\n",
    "  4. 使用目标数据集训练目标模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![模型微调流程](images/ch05/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用已有模型结构：通过传入`pretrained`参数，决定是否使用预训练好的权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 训练特定层：使用`requires_grad=False`冻结部分网络层，只计算新初始化的层的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "# 冻结参数的梯度\n",
    "feature_extract = True\n",
    "model = models.resnet50(pretrained=True)\n",
    "set_parameter_requires_grad(model, feature_extract)\n",
    "# 修改模型\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features=512, out_features=4, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=512, out_features=4, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：在训练过程中，model仍会回传梯度，但是参数更新只会发生在`fc`层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 半精度训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 半精度优势：减少显存占用，提高GPU同时加载的数据量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置半精度训练：\n",
    "  1. 导入`torch.cuda.amp`的`autocast`包\n",
    "  2. 在模型定义中的`forward`函数上，设置`autocast`装饰器\n",
    "  3. 在训练过程中，在数据输入模型之后，添加`with autocast()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 适用范围：适用于数据的size较大的数据集（比如3D图像、视频等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次任务，主要介绍了PyTorch的进阶训练技巧，包括自定义损失函数、动态调整学习率、模型微调和半精度训练等技巧。\n",
    "1. 自定义损失函数可以通过二种方式：函数方式和类方式，建议全程使用PyTorch提供的张量计算方法。\n",
    "2. 通过使用PyTorch中的scheduler动态调整学习率，也支持自定义scheduler\n",
    "3. 模型微调主要使用已有的预训练模型，调整其中的参数构建目标模型，在目标数据集上训练模型。\n",
    "4. 半精度训练主要适用于数据的size较大的数据集（比如3D图像、视频等）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
