{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task02 PyTorch主要组成模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 深度学习步骤\n",
    "\n",
    "（1）数据预处理：通过专门的数据加载，通过批训练提高模型表现，每次训练读取固定数量的样本输入到模型中进行训练  \n",
    "（2）深度神经网络搭建：逐层搭建，实现特定功能的层（如积层、池化层、批正则化层、LSTM层等）  \n",
    "（3）损失函数和优化器的设定：保证反向传播能够在用户定义的模型结构上实现  \n",
    "（4）模型训练：使用并行计算加速训练，将数据按批加载，放入GPU中训练，对损失函数反向传播回网络最前面的层，同时使用优化器调整网络参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 基本配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 导入相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 统一设置超参数：batch size、初始学习率、训练次数、GPU配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始学习率\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练次数\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置GPU\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 数据读入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 读取方式：通过Dataset+DataLoader的方式加载数据，Dataset定义好数据的格式和数据变换形式，DataLoader用iterative的方式不断读入批次数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自定义Dataset类：实现`__init___`、`__getitem__`、`__len__`函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `torch.utils.data.DataLoader`参数：\n",
    "  1. batch_size：样本是按“批”读入的，表示每次读入的样本数\n",
    "  2. num_workers：表示用于读取数据的进程数\n",
    "  3. shuffle：是否将读入的数据打乱\n",
    "  4. drop_last：对于样本最后一部分没有达到批次数的样本，使其不再参与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 神经网络的构造\n",
    "\n",
    "通过`Module`类构造模型，实例化模型之后，可完成模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造多层感知机\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        o = self.act(self.hidden(x))\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8924, 0.9624, 0.3262,  ..., 0.8376, 0.1889, 0.9060],\n",
      "        [0.3609, 0.8005, 0.5175,  ..., 0.6255, 0.1462, 0.9846]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0902,  0.0199,  0.0677, -0.0679,  0.0799,  0.0826,  0.0628,  0.1809,\n",
       "         -0.2387,  0.0366],\n",
       "        [-0.2271,  0.0056, -0.0984, -0.0432, -0.0160, -0.0038,  0.0953,  0.0545,\n",
       "         -0.1530, -0.0214]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 784)\n",
    "net = MLP()\n",
    "print(x)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 神经网络常见的层\n",
    "\n",
    "- 不含模型参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造一个输入减去均值后输出的层\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-10.,  -5.,   0.,   5.,  10.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 5, 10, 15, 20], dtype=torch.float)\n",
    "layer = MyLayer()\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 含模型参数的层：如果一个`Tensor`是`Parameter`，那么它会⾃动被添加到模型的参数列表里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用ParameterList定义参数的列表\n",
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用ParameterDict定义参数的字典\n",
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        # 新增参数linear3\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) \n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二维卷积层：使用`nn.Conv2d`类构造，模型参数包括卷积核和标量偏差，在训练模式时，通常先对卷积核随机初始化，再不断迭代卷积核和偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算卷积层，对输入和输出做相应的升维和降维\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1)代表批量大小和通道数\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 排除不关心的前两维：批量和通道\n",
    "    return Y.view(Y.shape[2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)\n",
    "\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 池化层：直接计算池化窗口内元素的最大值或者平均值，分别叫做最大池化或平均池化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二维池化层\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2), 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 模型示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络训练过程：\n",
    "  1. 定义可学习参数的神经网络\n",
    "  2. 在输入数据集上进行迭代训练\n",
    "  3. 通过神经网络处理输入数据\n",
    "  4. 计算loss（损失值）\n",
    "  5. 将梯度反向传播给神经网络参数\n",
    "  6. 更新网络参数，使用梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LeNet(前馈神经网络)\n",
    "![LeNet](images/ch02/01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像channel是1；输出channel是6；5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2x2 Max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果是方阵,则可以只使用一个数字进行定义\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # 除去批处理维度的其他所有维度\n",
    "        size = x.size()[1:]  \n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0921, -0.0605, -0.0726, -0.0451,  0.1399, -0.0087,  0.1075,  0.0799,\n",
      "         -0.1472,  0.0288]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\LearningDisk\\Learning_Projects\\MyPythonProjects\\my-team-learning\\venv\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# 假设输入的数据为随机的32x32\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清零所有参数的梯度缓存，然后进行随机梯度的反向传播\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet\n",
    "![AlexNet](images/ch02/02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # in_channels, out_channels, kernel_size, stride, padding\n",
    "            nn.Conv2d(1, 96, 11, 4), \n",
    "            nn.ReLU(),\n",
    "            # kernel_size, stride\n",
    "            nn.MaxPool2d(3, 2), \n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            # 连续3个卷积层，且使用更小的卷积窗口。\n",
    "            # 除了最后的卷积层外，进一步增大了输出通道数。\n",
    "            # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "            nn.Linear(4096, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二分类交叉熵损失函数：`torch.nn.BCELoss`，用于计算二分类任务时的交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE损失函数的计算结果: tensor(0.9389, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "output = loss(m(input), target)\n",
    "output.backward()\n",
    "print('BCE损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵损失函数：`torch.nn.CrossEntropyLoss`，用于计算交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy损失函数的计算结果: tensor(2.7367, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('CrossEntropy损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L1损失函数：`torch.nn.L1Loss`，用于计算输出`y`和真实值`target`之差的绝对值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1损失函数的计算结果: tensor(1.0351, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.L1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('L1损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MSE损失函数：`torch.nn.MSELoss`，用于计算输出`y`和真实值`target`之差的平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE损失函数的计算结果: tensor(1.7612, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MSELoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('MSE损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 平滑L1（Smooth L1）损失函数：`torch.nn.SmoothL1Loss`，用于计算L1的平滑输出，减轻离群点带来的影响，通过与L1损失的比较，在0点的尖端处，过渡更为平滑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smooth L1损失函数的计算结果: tensor(0.7252, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.SmoothL1Loss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print('Smooth L1损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标泊松分布的负对数似然损失：`torch.nn.PoissonNLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonNL损失函数的计算结果: tensor(1.7593, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.PoissonNLLLoss()\n",
    "log_input = torch.randn(5, 2, requires_grad=True)\n",
    "target = torch.randn(5, 2)\n",
    "\n",
    "output = loss(log_input, target)\n",
    "output.backward()\n",
    "print('PoissonNL损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KL散度：`torch.nn.KLDivLoss`，用于连续分布的距离度量，可用在对离散采用的连续输出空间分布的回归场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLDiv损失函数的计算结果: tensor(-1.0006)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.5, 0.3, 0.2], [0.2, 0.3, 0.5]])\n",
    "target = torch.tensor([[0.9, 0.05, 0.05], [0.1, 0.7, 0.2]], dtype=torch.float)\n",
    "loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "output = loss(inputs,target)\n",
    "print('KLDiv损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MarginRankingLoss：`torch.nn.MarginRankingLoss`，用于计算两组数据之间的差异（相似度），可使用在排序任务的场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarginRanking损失函数的计算结果: tensor(1.1762, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MarginRankingLoss()\n",
    "input1 = torch.randn(3, requires_grad=True)\n",
    "input2 = torch.randn(3, requires_grad=True)\n",
    "target = torch.randn(3).sign()\n",
    "\n",
    "output = loss(input1, input2, target)\n",
    "output.backward()\n",
    "print('MarginRanking损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多标签边界损失函数：`torch.nn.MultiLabelMarginLoss`，用于计算多标签分类问题的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLabelMargin损失函数的计算结果: tensor(0.4500)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.MultiLabelMarginLoss()\n",
    "x = torch.FloatTensor([[0.9, 0.2, 0.4, 0.8]])\n",
    "# 真实的分类是，第3类和第0类\n",
    "y = torch.LongTensor([[3, 0, -1, 1]])\n",
    "\n",
    "output = loss(x, y)\n",
    "print('MultiLabelMargin损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二分类损失函数：`torch.nn.SoftMarginLoss`，用于计算二分类的`logistic`损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftMargin损失函数的计算结果: tensor(0.6764)\n"
     ]
    }
   ],
   "source": [
    "# 定义两个样本，两个神经元\n",
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]])  \n",
    "target = torch.tensor([[-1, 1], [1, -1]], dtype=torch.float)  \n",
    "\n",
    "# 该loss对每个神经元计算，需要为每个神经元单独设置标签\n",
    "loss_f = nn.SoftMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "print('SoftMargin损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 多分类的折页损失函数：`torch.nn.MultiMarginLoss`，用于计算多分类问题的折页损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiMargin损失函数的计算结果: tensor(0.6000)\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[0.3, 0.7], [0.5, 0.5]]) \n",
    "target = torch.tensor([0, 1], dtype=torch.long) \n",
    "\n",
    "loss_f = nn.MultiMarginLoss()\n",
    "output = loss_f(inputs, target)\n",
    "print('MultiMargin损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三元组损失函数：`torch.nn.TripletMarginLoss`，用于处理<实体1，关系，实体2>类型的数据，计算该类型数据的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TripletMargin损失函数的计算结果: tensor(1.1507, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "anchor = torch.randn(100, 128, requires_grad=True)\n",
    "positive = torch.randn(100, 128, requires_grad=True)\n",
    "negative = torch.randn(100, 128, requires_grad=True)\n",
    "\n",
    "output = triplet_loss(anchor, positive, negative)\n",
    "output.backward()\n",
    "print('TripletMargin损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HingEmbeddingLoss：`torch.nn.HingeEmbeddingLoss`，用于计算输出的embedding结果的Hing损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HingEmbedding损失函数的计算结果: tensor(0.7667)\n"
     ]
    }
   ],
   "source": [
    "loss_f = nn.HingeEmbeddingLoss()\n",
    "inputs = torch.tensor([[1., 0.8, 0.5]])\n",
    "target = torch.tensor([[1, 1, -1]])\n",
    "\n",
    "output = loss_f(inputs,target)\n",
    "print('HingEmbedding损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 余弦相似度：`torch.nn.CosineEmbeddingLoss`，用于计算两个向量的余弦相似度，如果两个向量距离近，则损失函数值小，反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CosineEmbedding损失函数的计算结果: tensor(0.5000)\n"
     ]
    }
   ],
   "source": [
    "loss_f = nn.CosineEmbeddingLoss()\n",
    "inputs_1 = torch.tensor([[0.3, 0.5, 0.7], [0.3, 0.5, 0.7]])\n",
    "inputs_2 = torch.tensor([[0.1, 0.3, 0.5], [0.1, 0.3, 0.5]])\n",
    "target = torch.tensor([1, -1], dtype=torch.float)\n",
    "\n",
    "output = loss_f(inputs_1,inputs_2,target)\n",
    "print('CosineEmbedding损失函数的计算结果:',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CTC损失函数：`torch.nn.CTCLoss`，用于处理时序数据的分类问题，计算连续时间序列和目标序列之间的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC损失函数的计算结果: tensor(6.1333, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Target are to be padded\n",
    "# 序列长度\n",
    "T = 50      \n",
    "# 类别数（包括空类）\n",
    "C = 20      \n",
    "# batch size\n",
    "N = 16\n",
    "# Target sequence length of longest target in batch (padding length)\n",
    "S = 30      \n",
    "# Minimum target length, for demonstration purposes\n",
    "S_min = 10  \n",
    "\n",
    "input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n",
    "# 初始化target(0 = blank, 1:C = classes)\n",
    "target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n",
    "\n",
    "input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n",
    "target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n",
    "\n",
    "ctc_loss = nn.CTCLoss()\n",
    "loss = ctc_loss(input, target, input_lengths, target_lengths)\n",
    "loss.backward()\n",
    "print('CTC损失函数的计算结果:',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Optimizer的属性和方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用方向：为了使求解参数过程更快，使用BP+优化器逼近求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizer的属性：\n",
    "  - `defaults`：优化器的超参数\n",
    "  - `state`：参数的缓存\n",
    "  - `param_groups`：参数组，顺序是params，lr，momentum，dampening，weight_decay，nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Optimizer的方法：\n",
    "  - `zero_grad()`：清空所管理参数的梯度\n",
    "  - `step()`：执行一步梯度更新\n",
    "  - `add_param_group()`：添加参数组\n",
    "  - `load_state_dict()`：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练\n",
    "  - `state_dict()`：获取优化器当前状态信息字典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight before step:\n",
      "tensor([[-0.5871, -1.1311],\n",
      "        [-1.0446,  0.2656]])\n",
      "The grad of weight before step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data of weight after step:\n",
      "tensor([[-0.6871, -1.2311],\n",
      "        [-1.1446,  0.1656]])\n",
      "The grad of weight after step:\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grad of weight after optimizer.zero_grad():\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[-0.6871, -1.2311],\n",
      "        [-1.1446,  0.1656]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.0411, -0.6569,  0.7445],\n",
      "        [-0.7056,  1.1146, -0.4409],\n",
      "        [-0.2302, -1.1507, -1.3807]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n",
      "state_dict before step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
      "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict after step:\n",
      " {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
      "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n"
     ]
    }
   ],
   "source": [
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # 设置训练状态\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # 循环读取DataLoader中的全部数据\n",
    "    for data, label in train_loader:\n",
    "        # 将数据放到GPU用于后续计算\n",
    "        data, label = data.cuda(), label.cuda()\n",
    "        # 将优化器的梯度清0\n",
    "        optimizer.zero_grad()\n",
    "        # 将数据输入给模型\n",
    "        output = model(data)\n",
    "        # 设置损失函数\n",
    "        loss = criterion(label, output)\n",
    "        # 将loss反向传播给网络\n",
    "        loss.backward()\n",
    "        # 使用优化器更新模型参数\n",
    "        optimizer.step()\n",
    "        # 累加训练损失\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(epoch):  \n",
    "    # 设置验证状态\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    # 不设置梯度\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            preds = torch.argmax(output, 1)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()*data.size(0)\n",
    "            # 计算准确率\n",
    "            running_accu += torch.sum(preds == label.data)\n",
    "    val_loss = val_loss/len(val_loader.dataset)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次任务，通过介绍PyTorch的主要组成模块，使用PyTorch框架进行深度学习，详细介绍了深度学习的各个环节，包括数据加载、模型构建、损失函数、优化器、训练与评估。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
