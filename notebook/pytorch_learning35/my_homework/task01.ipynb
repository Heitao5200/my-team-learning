{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task01 PyTorch简介与基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 PyTorch简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 概念：由Facebook人工智能研究小组开发的一种基于Lua编写的Torch库的Python实现的深度学习库\n",
    "- 优势：简洁、上手快、具有良好的文档和社区支持、项目开源、支持代码调试、丰富的扩展库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PyTorch基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分类：0维张量（标量）、1维张量（向量）、2维张量（矩阵）、3维张量（时间序列）、4维张量（图像）、5维张量（视频）\n",
    "- 概念：一个数据容器，可以包含数据、字符串等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9515, 0.6332, 0.8228],\n",
      "        [0.3508, 0.0493, 0.7606],\n",
      "        [0.7326, 0.7003, 0.1925],\n",
      "        [0.1172, 0.8946, 0.9501]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 创建tensor\n",
    "x = torch.rand(4, 3)\n",
    "print(x)\n",
    "# 构造数据类型为long，数据是0的矩阵\n",
    "x = torch.zeros(4, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 常见的构造Tensor的函数：\n",
    "|                  函数 | 功能                             |\n",
    "| --------------------:| ------------------------------------------------------- |\n",
    "| Tensor(\\**sizes*)    | 基础构造函数                             |\n",
    "| tensor(*data*)      | 类似于np.array                            |\n",
    "| ones(\\**sizes*)      | 全1                                   |\n",
    "| zeros(\\**sizes*)    | 全0                                    |\n",
    "| eye(\\**sizes*)      | 对角为1，其余为0                           |\n",
    "| arange(*s,e,step*)   | 从s到e，步长为step                         |\n",
    "| linspace(*s,e,steps*) | 从s到e，均匀分成step份                       |\n",
    "| rand/randn(\\**sizes*) | rand是\\[0,1)均匀分布；randn是服从N（0，1）的正态分布   |\n",
    "| normal(*mean,std*)   | 正态分布（均值为mean，标准差是std）              |\n",
    "| randperm(*m*)      | 随机排列                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 操作：\n",
    "  1. 使用索引表示的变量与原数据共享内存，即修改其中一个，另一个也会被修改\n",
    "  2. 使用`torch.view`改变tensor的大小\n",
    "  3. 广播机制：当对两个形状不同的Tensor按元素运算时，可能会触发广播(broadcasting)机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4]) torch.Size([20]) torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 使用view改变张量的大小\n",
    "x = torch.randn(5, 4)\n",
    "y = x.view(20)\n",
    "z = x.view(-1, 5) # -1是指这一维的维数由其他维度决定\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[1, 2]])\n",
      "y = tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "x + y = tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(\"x =\", x)\n",
    "print(\"y =\", y)\n",
    "print(\"x + y =\", x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `autograd`包：提供张量上的自动求导机制\n",
    "- 原理：如果设置`.requires_grad`为`True`，那么将会追踪张量的所有操作。当完成计算后，可以通过调用`.backward()`自动计算所有的梯度。张量的所有梯度将会自动累加到`.grad`属性\n",
    "- `Function`：`Tensor`和`Function`互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个`.grad_fn`属性，该属性引用了创建`Tensor`自身的`Function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<MulBackward0>)\n",
      "z mean =  tensor(3., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(\"z = \", z)\n",
    "print(\"z mean = \", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 梯度：对于那么 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度就是一个雅可比矩阵\n",
    "$$\n",
    "J=\\left(\n",
    "\\begin{array}{ccc}\n",
    "\\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `grad`的反向传播：运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# 反向传播累加\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 并行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目的：通过使用多个GPU参与训练，加快训练速度，提高模型学习的效果\n",
    "- CUDA：通过使用NVIDIA提供的GPU并行计算框架，采用`cuda()`方法，让模型或者数据迁移到GPU中进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 并行计算方法：\n",
    "  1. Network partitioning：将一个模型网络的各部分拆分，分配到不同的GPU中,执行不同的计算任务\n",
    "  2. Layer-wise partitioning：将同一层模型拆分，分配到不同的GPU中，训练同一层模型的部分任务\n",
    "  3. Data parallelism（主流）：将不同的数据分配到不同的GPU中，执行相同的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;本次任务，主要介绍了PyTorch概念及优势、以及基础知识，包括张量、自动求导和并行计算；通过构建张量，存储我们需要的数据；基于自动求导机制和雅可比矩阵的计算规则，计算张量的梯度；并行计算方法主要包括Network partitioning、Layer-wise partitioning和Data parallelism，目前主流的是最后一种。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
